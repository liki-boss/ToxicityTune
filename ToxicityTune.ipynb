{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3155498c-fd83-424c-9950-85eaac224811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/ubuntu/.local/lib/python3.8/site-packages (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (2023.8.8)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.8/site-packages (from nltk) (4.64.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06224389-52c3-4d8d-ba34-55922c76e08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2023.8.8)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (2.28.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.3.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m132.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (23.0)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m120.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from transformers) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (0.6.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (2.8)\n",
      "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0a0f32d-9db1-4796-bce7-3f2307794688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fsspec[http]>=2021.11.1\n",
      "  Downloading fsspec-2023.6.0-py3-none-any.whl (163 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/163.8 kB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (1.5.3)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (23.0)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m122.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.3.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/194.6 kB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (2.28.2)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.0/39.0 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (1.23.5)\n",
      "Collecting dill<0.3.8,>=0.3.0\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (266 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m266.9/266.9 kB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (121 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.3/121.3 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (220 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.1/220.1 kB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "Installing collected packages: xxhash, pyarrow, multidict, fsspec, frozenlist, dill, async-timeout, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "Successfully installed aiohttp-3.8.5 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.14.4 dill-0.3.7 frozenlist-1.4.0 fsspec-2023.6.0 multidict-6.0.4 multiprocess-0.70.15 pyarrow-12.0.1 xxhash-3.3.0 yarl-1.9.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "628afae4-9e33-4bae-822a-2fe1c76e80c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.8/site-packages (2.14.4)\n",
      "Requirement already satisfied: fsspec in /home/ubuntu/.local/lib/python3.8/site-packages (2023.6.0)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (2.28.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.16.4)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: aiohttp in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (3.8.5)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.3.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: multiprocess in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ubuntu/.local/lib/python3.8/site-packages (from datasets) (1.23.5)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (19.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (2019.11.28)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->datasets) (2022.7.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade datasets fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f0a5b44",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 14.575988,
     "end_time": "2023-08-09T06:23:39.521904",
     "exception": false,
     "start_time": "2023-08-09T06:23:24.945916",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/pandas/core/computation/expressions.py:20: UserWarning: Pandas requires version '2.7.3' or newer of 'numexpr' (version '2.7.1' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "[nltk_data] Downloading package punkt to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import scipy\n",
    "import string\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import itertools\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from transformers import BertModel, BertTokenizerFast, BertConfig, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from datasets import Dataset, load_dataset, DatasetDict\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics, model_selection, linear_model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "## set seeds for repeatable conclusion\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "tqdm.pandas()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2d7bdc8-8f5b-4ad2-9ac0-da2e1d3b1bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc3c2ce-065b-4251-8eb8-39ed201f0841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opendatasets\n",
      "  Downloading opendatasets-0.1.22-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.8/site-packages (from opendatasets) (4.64.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from opendatasets) (7.0)\n",
      "Collecting kaggle\n",
      "  Downloading kaggle-1.5.16.tar.gz (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: bleach in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (3.1.1)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /home/ubuntu/.local/lib/python3.8/site-packages (from kaggle->opendatasets) (2.8.2)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-8.0.1-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.8/site-packages (from kaggle->opendatasets) (2.28.2)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (1.14.0)\n",
      "Requirement already satisfied: urllib3 in /usr/lib/python3/dist-packages (from kaggle->opendatasets) (1.25.8)\n",
      "Collecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.8/site-packages (from requests->kaggle->opendatasets) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kaggle->opendatasets) (2.8)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.16-py3-none-any.whl size=110688 sha256=ab0b9165e352c71cb35a1e05d651842e253c77ab4a8293f080e3d255f48f846f\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/5a/ab/50/e224f599a07faf6d398a8600796012da271b7e5e7f2a3ab2b8\n",
      "Successfully built kaggle\n",
      "Installing collected packages: text-unidecode, python-slugify, kaggle, opendatasets\n",
      "Successfully installed kaggle-1.5.16 opendatasets-0.1.22 python-slugify-8.0.1 text-unidecode-1.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pandas in /home/ubuntu/.local/lib/python3.8/site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/ubuntu/.local/lib/python3.8/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas) (1.14.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install opendatasets\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a25ba691-88e6-4af7-91a3-556f87f0a9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading jigsaw-toxic-comment-classification-challenge.zip to ./jigsaw-toxic-comment-classification-challenge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 53.4M/53.4M [00:00<00:00, 113MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import opendatasets as od\n",
    "import pandas\n",
    " \n",
    "od.download(\n",
    "    \"https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba40dc9b",
   "metadata": {
    "papermill": {
     "duration": 0.008787,
     "end_time": "2023-08-09T06:23:39.539787",
     "exception": false,
     "start_time": "2023-08-09T06:23:39.531000",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Peeking into the Data and reviewing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62a1f8fa",
   "metadata": {
    "papermill": {
     "duration": 0.128832,
     "end_time": "2023-08-09T06:23:39.677302",
     "exception": false,
     "start_time": "2023-08-09T06:23:39.548470",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7537 entries, 0 to 7536\n",
      "Data columns (total 2 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   comment_id  7537 non-null   int64 \n",
      " 1   text        7537 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 117.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "601                               silly little boy my ass\n",
       "2031    Is that so? Than why so many people questiong ...\n",
       "1928     Wow \\nThanks! You are SO amazing! I am in awe...\n",
       "468     \"\\n\\n Send this to User:Bumpusmills1 \\n\\nPass ...\n",
       "6351    \"\\n\\nthat is your opinion. And what is your pr...\n",
       "748      man its all about captain morgans spiced rum....\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = pd.read_csv('jigsaw-toxic-severity-rating/comments_to_score.csv')\n",
    "data_1.info()\n",
    "\n",
    "# Isolating only the text column\n",
    "data_1 = data_1['text']\n",
    "\n",
    "#Looking into how the data looks like for pre-processing\n",
    "data_1.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b715bcc1",
   "metadata": {
    "papermill": {
     "duration": 0.598488,
     "end_time": "2023-08-09T06:23:40.285062",
     "exception": false,
     "start_time": "2023-08-09T06:23:39.686574",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30108 entries, 0 to 30107\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   worker      30108 non-null  int64 \n",
      " 1   less_toxic  30108 non-null  object\n",
      " 2   more_toxic  30108 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 705.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>worker</th>\n",
       "      <th>less_toxic</th>\n",
       "      <th>more_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19699</th>\n",
       "      <td>193</td>\n",
       "      <td>I agree \\n\\nDayewalker is just another republ...</td>\n",
       "      <td>question \\n\\ncan I please ask you a question?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15268</th>\n",
       "      <td>240</td>\n",
       "      <td>Are there Turks live? LOL I have never get  re...</td>\n",
       "      <td>WHY DONT YOU FUCK OFF YOU MANWHORE AND LET US ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16859</th>\n",
       "      <td>652</td>\n",
       "      <td>Burn In Hell Paulley \\nEvery time I try to ed...</td>\n",
       "      <td>you fucking piece of shit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>513</td>\n",
       "      <td>LAWL \\n\\nlulz @ u nerd, keep deleting my comm...</td>\n",
       "      <td>John Cena's wrestling abilities\\nJohn Cena is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14019</th>\n",
       "      <td>527</td>\n",
       "      <td>Your vandalism \\nPlease stop. If you continue...</td>\n",
       "      <td>As for [http://en.wikipedia.org/w/index.php?ti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26733</th>\n",
       "      <td>201</td>\n",
       "      <td>LOL what a load of unsourced, unscientific bu...</td>\n",
       "      <td>Todo speedy deletion candidates \\n\\nI do not ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       worker                                         less_toxic  \\\n",
       "19699     193   I agree \\n\\nDayewalker is just another republ...   \n",
       "15268     240  Are there Turks live? LOL I have never get  re...   \n",
       "16859     652   Burn In Hell Paulley \\nEvery time I try to ed...   \n",
       "12496     513   LAWL \\n\\nlulz @ u nerd, keep deleting my comm...   \n",
       "14019     527   Your vandalism \\nPlease stop. If you continue...   \n",
       "26733     201   LOL what a load of unsourced, unscientific bu...   \n",
       "\n",
       "                                              more_toxic  \n",
       "19699   question \\n\\ncan I please ask you a question?...  \n",
       "15268  WHY DONT YOU FUCK OFF YOU MANWHORE AND LET US ...  \n",
       "16859                         you fucking piece of shit   \n",
       "12496  John Cena's wrestling abilities\\nJohn Cena is ...  \n",
       "14019  As for [http://en.wikipedia.org/w/index.php?ti...  \n",
       "26733   Todo speedy deletion candidates \\n\\nI do not ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_2 = pd.read_csv('jigsaw-toxic-severity-rating/validation_data.csv')\n",
    "data_2.info()\n",
    "\n",
    "data_2.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "555fe829",
   "metadata": {
    "papermill": {
     "duration": 2.685735,
     "end_time": "2023-08-09T06:23:42.980588",
     "exception": false,
     "start_time": "2023-08-09T06:23:40.294853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 223549 entries, 0 to 223548\n",
      "Data columns (total 8 columns):\n",
      " #   Column         Non-Null Count   Dtype \n",
      "---  ------         --------------   ----- \n",
      " 0   id             223549 non-null  object\n",
      " 1   comment_text   223549 non-null  object\n",
      " 2   toxic          223549 non-null  int64 \n",
      " 3   severe_toxic   223549 non-null  int64 \n",
      " 4   obscene        223549 non-null  int64 \n",
      " 5   threat         223549 non-null  int64 \n",
      " 6   insult         223549 non-null  int64 \n",
      " 7   identity_hate  223549 non-null  int64 \n",
      "dtypes: int64(6), object(2)\n",
      "memory usage: 13.6+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60312</th>\n",
       "      <td>a173202a7f9111e8</td>\n",
       "      <td>No you understand me uncorrectly because your ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26185</th>\n",
       "      <td>454e638656078801</td>\n",
       "      <td>WTF is your problem?</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219850</th>\n",
       "      <td>f0c6a44e6c371cba</td>\n",
       "      <td>== Moody's is a shit == \\n\\n Moody's is a stup...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114115</th>\n",
       "      <td>62810646b8a80daf</td>\n",
       "      <td>this is fucking retarded \\n\\nTreating flies, m...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8365</th>\n",
       "      <td>1638067cd3655acd</td>\n",
       "      <td>\", 31 March 2011 (UTC)\\n\\n See you jackasses a...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>07b36c594fbd6b54</td>\n",
       "      <td>@ okay king of the Wikipedia Nazis.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "60312   a173202a7f9111e8  No you understand me uncorrectly because your ...   \n",
       "26185   454e638656078801                               WTF is your problem?   \n",
       "219850  f0c6a44e6c371cba  == Moody's is a shit == \\n\\n Moody's is a stup...   \n",
       "114115  62810646b8a80daf  this is fucking retarded \\n\\nTreating flies, m...   \n",
       "8365    1638067cd3655acd  \", 31 March 2011 (UTC)\\n\\n See you jackasses a...   \n",
       "2854    07b36c594fbd6b54                @ okay king of the Wikipedia Nazis.   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  sum  \n",
       "60312       1             0        0       0       0              0    1  \n",
       "26185       1             0        0       0       0              0    1  \n",
       "219850      1             0        1       0       1              0    3  \n",
       "114115      1             0        1       0       0              0    2  \n",
       "8365        1             0        0       0       0              0    1  \n",
       "2854        1             0        0       0       0              0    1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_4 = pd.read_csv('toxic-comment/jigsaw-toxic-comment-train.csv')\n",
    "data_4.info()\n",
    "\n",
    "## Sum across labels to filter out non-toxic comments\n",
    "data_4['sum'] = data_4.loc[:, 'toxic':].sum(axis=1)\n",
    "\n",
    "## Keep only comments with some type of label that categorizes them as toxic\n",
    "data_4 = data_4[data_4['sum'] > 0]\n",
    "\n",
    "data_4.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afe21331",
   "metadata": {
    "papermill": {
     "duration": 25.354415,
     "end_time": "2023-08-09T06:24:08.347711",
     "exception": false,
     "start_time": "2023-08-09T06:23:42.993296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1902194 entries, 0 to 1902193\n",
      "Data columns (total 45 columns):\n",
      " #   Column                               Dtype  \n",
      "---  ------                               -----  \n",
      " 0   id                                   int64  \n",
      " 1   comment_text                         object \n",
      " 2   toxic                                float64\n",
      " 3   severe_toxicity                      float64\n",
      " 4   obscene                              float64\n",
      " 5   identity_attack                      float64\n",
      " 6   insult                               float64\n",
      " 7   threat                               float64\n",
      " 8   asian                                float64\n",
      " 9   atheist                              float64\n",
      " 10  bisexual                             float64\n",
      " 11  black                                float64\n",
      " 12  buddhist                             float64\n",
      " 13  christian                            float64\n",
      " 14  female                               float64\n",
      " 15  heterosexual                         float64\n",
      " 16  hindu                                float64\n",
      " 17  homosexual_gay_or_lesbian            float64\n",
      " 18  intellectual_or_learning_disability  float64\n",
      " 19  jewish                               float64\n",
      " 20  latino                               float64\n",
      " 21  male                                 float64\n",
      " 22  muslim                               float64\n",
      " 23  other_disability                     float64\n",
      " 24  other_gender                         float64\n",
      " 25  other_race_or_ethnicity              float64\n",
      " 26  other_religion                       float64\n",
      " 27  other_sexual_orientation             float64\n",
      " 28  physical_disability                  float64\n",
      " 29  psychiatric_or_mental_illness        float64\n",
      " 30  transgender                          float64\n",
      " 31  white                                float64\n",
      " 32  created_date                         object \n",
      " 33  publication_id                       int64  \n",
      " 34  parent_id                            float64\n",
      " 35  article_id                           int64  \n",
      " 36  rating                               object \n",
      " 37  funny                                int64  \n",
      " 38  wow                                  int64  \n",
      " 39  sad                                  int64  \n",
      " 40  likes                                int64  \n",
      " 41  disagree                             int64  \n",
      " 42  sexual_explicit                      float64\n",
      " 43  identity_annotator_count             int64  \n",
      " 44  toxicity_annotator_count             int64  \n",
      "dtypes: float64(32), int64(10), object(3)\n",
      "memory usage: 653.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data_5 = pd.read_csv('toxic-comment/jigsaw-unintended-bias-train.csv')\n",
    "data_5.info()\n",
    "\n",
    "drop_columns = [\n",
    "    'created_date',\n",
    "    'publication_id',\n",
    "    'parent_id',\n",
    "    'article_id', \n",
    "    'rating', \n",
    "    'funny', \n",
    "    'wow', \n",
    "    'sad', \n",
    "    'likes', \n",
    "    'disagree', \n",
    "    'identity_annotator_count', \n",
    "    'toxicity_annotator_count'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b83cb47e",
   "metadata": {
    "papermill": {
     "duration": 1.516679,
     "end_time": "2023-08-09T06:24:09.874947",
     "exception": false,
     "start_time": "2023-08-09T06:24:08.358268",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>sum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1774450</th>\n",
       "      <td>6296060</td>\n",
       "      <td>We are in a war, jusris.  It is being waged at...</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>292344</th>\n",
       "      <td>599996</td>\n",
       "      <td>From my days in the modules, most criminals ar...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>356883</th>\n",
       "      <td>680207</td>\n",
       "      <td>Clearly there were lots of self deluded voters...</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215784</th>\n",
       "      <td>5601158</td>\n",
       "      <td>I wonder what you got dinged for?</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133702</th>\n",
       "      <td>405444</td>\n",
       "      <td>Blind allegiance.  A 10 year old girl would in...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1405961</th>\n",
       "      <td>5836878</td>\n",
       "      <td>Junior's Government must have forgotten that \"...</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id                                       comment_text     toxic  \\\n",
       "1774450  6296060  We are in a war, jusris.  It is being waged at...  0.166667   \n",
       "292344    599996  From my days in the modules, most criminals ar...  0.200000   \n",
       "356883    680207  Clearly there were lots of self deluded voters...  0.200000   \n",
       "1215784  5601158                  I wonder what you got dinged for?  0.400000   \n",
       "133702    405444  Blind allegiance.  A 10 year old girl would in...  0.000000   \n",
       "1405961  5836878  Junior's Government must have forgotten that \"...  0.300000   \n",
       "\n",
       "         severe_toxicity  obscene  identity_attack    insult  threat  asian  \\\n",
       "1774450              0.0      0.0         0.166667  0.166667     0.0    NaN   \n",
       "292344               0.0      0.0         0.000000  0.200000     0.0    NaN   \n",
       "356883               0.0      0.0         0.000000  0.000000     0.0    0.0   \n",
       "1215784              0.0      0.0         0.000000  0.000000     0.0    NaN   \n",
       "133702               0.0      0.0         0.000000  0.000000     0.0    0.0   \n",
       "1405961              0.0      0.0         0.100000  0.300000     0.0    NaN   \n",
       "\n",
       "         atheist  ...  other_gender  other_race_or_ethnicity  other_religion  \\\n",
       "1774450      NaN  ...           NaN                      NaN             NaN   \n",
       "292344       NaN  ...           NaN                      NaN             NaN   \n",
       "356883       0.0  ...           0.1                      0.0             0.0   \n",
       "1215784      NaN  ...           NaN                      NaN             NaN   \n",
       "133702       0.0  ...           0.0                      0.0             0.0   \n",
       "1405961      NaN  ...           NaN                      NaN             NaN   \n",
       "\n",
       "         other_sexual_orientation  physical_disability  \\\n",
       "1774450                       NaN                  NaN   \n",
       "292344                        NaN                  NaN   \n",
       "356883                        0.0             0.000000   \n",
       "1215784                       NaN                  NaN   \n",
       "133702                        0.0             0.166667   \n",
       "1405961                       NaN                  NaN   \n",
       "\n",
       "         psychiatric_or_mental_illness  transgender  white  sexual_explicit  \\\n",
       "1774450                            NaN          NaN    NaN              0.0   \n",
       "292344                             NaN          NaN    NaN              0.0   \n",
       "356883                             0.0          0.0    0.0              0.0   \n",
       "1215784                            NaN          NaN    NaN              0.4   \n",
       "133702                             0.0          0.0    0.0              0.0   \n",
       "1405961                            NaN          NaN    NaN              0.1   \n",
       "\n",
       "              sum  \n",
       "1774450  0.500000  \n",
       "292344   0.400000  \n",
       "356883   0.700000  \n",
       "1215784  0.800000  \n",
       "133702   0.333333  \n",
       "1405961  0.800000  \n",
       "\n",
       "[6 rows x 34 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The unintended bias training data-set has a few coloumns that isn't of our interest and hence we will be dropping them\n",
    "data_5 = data_5.drop(columns=drop_columns, axis=0)\n",
    "\n",
    "## Sum across labels to filter out clean comments\n",
    "data_5['sum'] = data_5.loc[:, 'toxic':].sum(axis=1)\n",
    "\n",
    "## Keep only comments with some type of label\n",
    "data_5 = data_5[data_5['sum'] > 0]\n",
    "\n",
    "data_5.sample(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b958314",
   "metadata": {
    "papermill": {
     "duration": 0.032833,
     "end_time": "2023-08-09T06:24:09.918561",
     "exception": false,
     "start_time": "2023-08-09T06:24:09.885728",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Isolating the text columns\n",
    "data_3 = data_2['less_toxic']\n",
    "data_2 = data_2['more_toxic']\n",
    "data_4 = data_4['comment_text']\n",
    "data_5 = data_5['comment_text']\n",
    "\n",
    "text_column = pd.concat([data_1, data_2, data_3, data_4, data_5], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fe3582",
   "metadata": {
    "papermill": {
     "duration": 0.01006,
     "end_time": "2023-08-09T06:24:09.939120",
     "exception": false,
     "start_time": "2023-08-09T06:24:09.929060",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As it goes with all datasets, there might be duplicates in the dataset.\n",
    "This being the first process of preparing the data for the purpose of training the model, removal of redundant data becomes a key process, which might otherwise cause the model to train inadequately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f74406c7",
   "metadata": {
    "papermill": {
     "duration": 0.922188,
     "end_time": "2023-08-09T06:24:10.871634",
     "exception": false,
     "start_time": "2023-08-09T06:24:09.949446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total numer of comments in text data = 797522\n",
      "Numer of unique comments in text data = 735613\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total numer of comments in text data = {len(text_column)}\")\n",
    "print(f\"Numer of unique comments in text data = {len(text_column.unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b414ed",
   "metadata": {
    "papermill": {
     "duration": 0.010257,
     "end_time": "2023-08-09T06:24:10.892653",
     "exception": false,
     "start_time": "2023-08-09T06:24:10.882396",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Upon observing the above unique comments, we have close to 62,000 duplicate values.\n",
    "We shall begin by dropping the duplicate values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a170e975",
   "metadata": {
    "papermill": {
     "duration": 0.430886,
     "end_time": "2023-08-09T06:24:11.334238",
     "exception": false,
     "start_time": "2023-08-09T06:24:10.903352",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate comments dropped\n"
     ]
    }
   ],
   "source": [
    "text_column = text_column.drop_duplicates()\n",
    "print(\"Duplicate comments dropped\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd1bbe1",
   "metadata": {
    "papermill": {
     "duration": 0.010478,
     "end_time": "2023-08-09T06:24:11.356019",
     "exception": false,
     "start_time": "2023-08-09T06:24:11.345541",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Exploring the toxic comments in the Dataset\n",
    "We shall now perform some simple tasks in order to get a better understanding of what the data looks like, if there's any correlations we can draw through plain sight to train the model more appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9edafefb",
   "metadata": {
    "papermill": {
     "duration": 1.336499,
     "end_time": "2023-08-09T06:24:12.703117",
     "exception": false,
     "start_time": "2023-08-09T06:24:11.366618",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_count</th>\n",
       "      <th>verb_count</th>\n",
       "      <th>noun_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>68.640000</td>\n",
       "      <td>11.640000</td>\n",
       "      <td>16.010000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>52.881136</td>\n",
       "      <td>9.327401</td>\n",
       "      <td>12.033199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>26.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>53.500000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>13.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>97.500000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>24.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>228.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>45.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_count  verb_count  noun_count\n",
       "count  100.000000  100.000000  100.000000\n",
       "mean    68.640000   11.640000   16.010000\n",
       "std     52.881136    9.327401   12.033199\n",
       "min      5.000000    0.000000    1.000000\n",
       "25%     26.500000    4.000000    6.000000\n",
       "50%     53.500000    9.000000   13.500000\n",
       "75%     97.500000   16.000000   24.000000\n",
       "max    228.000000   42.000000   45.000000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.DataFrame()\n",
    "data['text'] = text_column\n",
    "data = data.sample(100)\n",
    "\n",
    "# A Function to calculate the number of word\n",
    "def count_words(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    return len(words)\n",
    "\n",
    "# A Function to calculate the number of verb\n",
    "def count_verbs(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    verb_count = len([word for word, tag in tagged_words if tag.startswith('V')])\n",
    "    return verb_count\n",
    "\n",
    "# A Function to calculate the number of noun\n",
    "def count_nouns(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    noun_count = len([word for word, tag in tagged_words if tag.startswith('N')])\n",
    "    return noun_count\n",
    "\n",
    "# Add a column for the number of words, verbs and nouns\n",
    "data['word_count'] = data['text'].apply(count_words)\n",
    "data['verb_count'] = data['text'].apply(count_verbs)\n",
    "data['noun_count'] = data['text'].apply(count_nouns)\n",
    "\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a4adc8",
   "metadata": {
    "papermill": {
     "duration": 0.010889,
     "end_time": "2023-08-09T06:24:12.726065",
     "exception": false,
     "start_time": "2023-08-09T06:24:12.715176",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The purpose of the above code cell is to get an understanding of what type of data we would be working on. The size of the comments, the severity of the toxicity of the comment and if there are parameters that could be incorporated from the fragments of the sentence. We could visualise the above conclusions drawn also from the below graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8098f74a",
   "metadata": {
    "papermill": {
     "duration": 0.406708,
     "end_time": "2023-08-09T06:24:13.143634",
     "exception": false,
     "start_time": "2023-08-09T06:24:12.736926",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ax = data['word_count'].plot(kind='kde')\n",
    "data['verb_count'].plot(kind='kde', ax=ax)\n",
    "data['noun_count'].plot(kind='kde', ax=ax)\n",
    "\n",
    "ax.legend(['Word Count', 'Verb Count', 'Noun Count'])\n",
    "ax.set_title('Distribution of Word Count, Verb Count, and Noun Count')\n",
    "ax.set_xlabel('Count')\n",
    "ax.set_ylabel('Density')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5af19b",
   "metadata": {
    "papermill": {
     "duration": 0.011331,
     "end_time": "2023-08-09T06:24:13.166566",
     "exception": false,
     "start_time": "2023-08-09T06:24:13.155235",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can observe from the above plot that the comments are generally short, but also include relatively longer comments.\n",
    "\n",
    "At the present stage of the pre-processing, we haven't taken into consideration the misspelled words, which could be either intentional or unintentional. We shall rectify this at a later stage.\n",
    "\n",
    "Knowing the distributions behind some of these properties is crucial because, once the synthetic data is created, we'll probably want it to adhere to the same distributions for certain textual characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2be374f7",
   "metadata": {
    "papermill": {
     "duration": 0.192919,
     "end_time": "2023-08-09T06:24:13.371567",
     "exception": false,
     "start_time": "2023-08-09T06:24:13.178648",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bigrams</th>\n",
       "      <th>trigrams</th>\n",
       "      <th>quadgrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>((of, the), 31)</td>\n",
       "      <td>((you, do, n't), 5)</td>\n",
       "      <td>((., I, think, the), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>((in, the), 23)</td>\n",
       "      <td>((., If, you), 5)</td>\n",
       "      <td>((the, rest, of, the), 3)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>((is, a), 22)</td>\n",
       "      <td>((I, do, n't), 5)</td>\n",
       "      <td>((., If, you, mean), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>((do, n't), 19)</td>\n",
       "      <td>((., I, think), 4)</td>\n",
       "      <td>((is, one, of, the), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>((to, be), 17)</td>\n",
       "      <td>((to, be, the), 4)</td>\n",
       "      <td>((., Trump, is, a), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>((,, and), 17)</td>\n",
       "      <td>((., Trump, is), 4)</td>\n",
       "      <td>((?, You, do, n't), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>((., The), 15)</td>\n",
       "      <td>((I, think, the), 4)</td>\n",
       "      <td>((You, do, n't, see), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>((., I), 14)</td>\n",
       "      <td>((,, it, is), 3)</td>\n",
       "      <td>((was, in, the, wrong), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>((and, the), 14)</td>\n",
       "      <td>((,, and, the), 3)</td>\n",
       "      <td>((in, the, wrong, for), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>((to, the), 12)</td>\n",
       "      <td>((it, was, a), 3)</td>\n",
       "      <td>((the, media, has, only), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>((,, I), 12)</td>\n",
       "      <td>((is, a, winner), 3)</td>\n",
       "      <td>((media, has, only, focused), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>((,, but), 12)</td>\n",
       "      <td>((,, I, 'm), 3)</td>\n",
       "      <td>((has, only, focused, on), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>((I, 'm), 11)</td>\n",
       "      <td>((You, do, n't), 3)</td>\n",
       "      <td>((., It, 's, their), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>((you, do), 10)</td>\n",
       "      <td>((was, in, the), 3)</td>\n",
       "      <td>((a, winner, too, ,), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>((for, the), 10)</td>\n",
       "      <td>((in, the, wrong), 3)</td>\n",
       "      <td>((rest, of, the, world), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>((it, is), 10)</td>\n",
       "      <td>((., It, 's), 3)</td>\n",
       "      <td>((and, get, on, with), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>((., He), 10)</td>\n",
       "      <td>((the, rest, of), 3)</td>\n",
       "      <td>((get, on, with, life), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>((does, n't), 9)</td>\n",
       "      <td>((rest, of, the), 3)</td>\n",
       "      <td>((on, with, life, .), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>((., But), 9)</td>\n",
       "      <td>((if, you, do), 3)</td>\n",
       "      <td>((Do, oppressed, people, get), 2)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>((of, a), 9)</td>\n",
       "      <td>((a, crime, .), 3)</td>\n",
       "      <td>((if, you, do, n't), 2)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             bigrams               trigrams                          quadgrams\n",
       "0    ((of, the), 31)    ((you, do, n't), 5)            ((., I, think, the), 3)\n",
       "1    ((in, the), 23)      ((., If, you), 5)          ((the, rest, of, the), 3)\n",
       "2      ((is, a), 22)      ((I, do, n't), 5)            ((., If, you, mean), 2)\n",
       "3    ((do, n't), 19)     ((., I, think), 4)            ((is, one, of, the), 2)\n",
       "4     ((to, be), 17)     ((to, be, the), 4)             ((., Trump, is, a), 2)\n",
       "5     ((,, and), 17)    ((., Trump, is), 4)             ((?, You, do, n't), 2)\n",
       "6     ((., The), 15)   ((I, think, the), 4)           ((You, do, n't, see), 2)\n",
       "7       ((., I), 14)       ((,, it, is), 3)         ((was, in, the, wrong), 2)\n",
       "8   ((and, the), 14)     ((,, and, the), 3)         ((in, the, wrong, for), 2)\n",
       "9    ((to, the), 12)      ((it, was, a), 3)       ((the, media, has, only), 2)\n",
       "10      ((,, I), 12)   ((is, a, winner), 3)   ((media, has, only, focused), 2)\n",
       "11    ((,, but), 12)        ((,, I, 'm), 3)      ((has, only, focused, on), 2)\n",
       "12     ((I, 'm), 11)    ((You, do, n't), 3)            ((., It, 's, their), 2)\n",
       "13   ((you, do), 10)    ((was, in, the), 3)           ((a, winner, too, ,), 2)\n",
       "14  ((for, the), 10)  ((in, the, wrong), 3)        ((rest, of, the, world), 2)\n",
       "15    ((it, is), 10)       ((., It, 's), 3)          ((and, get, on, with), 2)\n",
       "16     ((., He), 10)   ((the, rest, of), 3)         ((get, on, with, life), 2)\n",
       "17  ((does, n't), 9)   ((rest, of, the), 3)           ((on, with, life, .), 2)\n",
       "18     ((., But), 9)     ((if, you, do), 3)  ((Do, oppressed, people, get), 2)\n",
       "19      ((of, a), 9)     ((a, crime, .), 3)            ((if, you, do, n't), 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['words'] = data['text'].apply(nltk.word_tokenize)\n",
    "\n",
    "# Get bigrams, trigrams and quadgrams for each row\n",
    "data['bigrams']   = data['words'].apply(lambda x: list(ngrams(x, 2)))\n",
    "data['trigrams']  = data['words'].apply(lambda x: list(ngrams(x, 3)))\n",
    "data['quadgrams'] = data['words'].apply(lambda x: list(ngrams(x, 4)))\n",
    "\n",
    "# Count the occurrences of bigrams, trigrams and quadgrams\n",
    "bigram_counts   = Counter([gram for grams in data['bigrams'] for gram in grams])\n",
    "trigram_counts  = Counter([gram for grams in data['trigrams'] for gram in grams])\n",
    "quadgram_counts = Counter([gram for grams in data['quadgrams'] for gram in grams])\n",
    "\n",
    "# Get the top 20 most common bigrams, trigrams, and quadgrams\n",
    "most_common_bigrams   = bigram_counts.most_common(20)\n",
    "most_common_trigrams  = trigram_counts.most_common(20)\n",
    "most_common_quadgrams = quadgram_counts.most_common(20)\n",
    "\n",
    "df_common_grams = pd.DataFrame()\n",
    "df_common_grams['bigrams']   = most_common_bigrams\n",
    "df_common_grams['trigrams']  = most_common_trigrams\n",
    "df_common_grams['quadgrams'] = most_common_quadgrams\n",
    "\n",
    "df_common_grams.iloc[:, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab74e6f9",
   "metadata": {
    "papermill": {
     "duration": 0.012335,
     "end_time": "2023-08-09T06:24:13.396209",
     "exception": false,
     "start_time": "2023-08-09T06:24:13.383874",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We will be ignoring the usual pre-processing step of converting the all characters to lower case as we would like the model to infer differences in the tone in which the comment was made. For instance some sentences/comments are made in all CAPS which usually indicate that the speaker is SHOUTING that comment, thereby treating that sentence as a toxic comment(although the severity is based on what was actually being said).\n",
    "\n",
    "It takes more than simply the literal meaning of the words to communicate meaning when training sentiment-based models, or models where mood and emotion are expressed in some form, such as toxicity of comments. Punctuation and capitalization are particularly expressive forms of language, thus it is preferable to leave them in the data for these issues. We could also apply this to why although the trigrams and quadgrams show punctuation marks as most common, it is in our interest to not drop them from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333f4bf0",
   "metadata": {
    "papermill": {
     "duration": 0.011276,
     "end_time": "2023-08-09T06:24:13.419189",
     "exception": false,
     "start_time": "2023-08-09T06:24:13.407913",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Pre-processing of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd45c98",
   "metadata": {
    "papermill": {
     "duration": 0.011324,
     "end_time": "2023-08-09T06:24:13.442011",
     "exception": false,
     "start_time": "2023-08-09T06:24:13.430687",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We would first like to deal with the mis-spelled words. For the purpose of this project, we would only be visiting and replacing words of those which we believe would effect the learning of the model greatly and ignore the rather less toxic and generally mis-spelled words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "805903d6",
   "metadata": {
    "papermill": {
     "duration": 0.029636,
     "end_time": "2023-08-09T06:24:13.483104",
     "exception": false,
     "start_time": "2023-08-09T06:24:13.453468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "RE_PATTERNS = {\n",
    "    ' fuck':\n",
    "        [\n",
    "            '(f)([^a-z]*)(u)([^a-z]*)(c)([^a-z]*)(k)',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*k',\n",
    "            ' f[!@#\\$%\\^\\&\\*]*u[!@#\\$%\\^&\\*]*[ck]+[a-z@#\\$%\\^&\\*]*'\n",
    "            'f u u c',\n",
    "            '(f)(c|[^a-z ])(u|[^a-z ])(k)',\n",
    "            'feck ', ' fux ', 'f\\*\\*', \n",
    "            'f\\-ing', 'f\\.u\\.', 'f###', ' fu ', 'f@ck', 'f u c k', 'f uck\\b', '\\bf ck\\b','\\bfuk\\b', 'wtf','fucck','f cking', 'fcking'\n",
    "        ],\n",
    "\n",
    "    ' ass ':\n",
    "        [\n",
    "            '[^a-z]ass ', '[^a-z]azz ', 'arrse', ' arse ', '@\\$\\$'\n",
    "                                                           '[^a-z]anus', ' a\\*s\\*s', '[^a-z]ass[^a-z ]',\n",
    "            'a[@#\\$%\\^&\\*][@#\\$%\\^&\\*]', '[^a-z]anal ', 'a s s'\n",
    "        ],\n",
    "\n",
    "    ' asshole ':\n",
    "        [\n",
    "            ' a[s|z]*wipe', 'a[s|z]*[w]*h[o|0]+[l]*e', '@\\$\\$hole', 'ass hole'\n",
    "        ],\n",
    "\n",
    "    ' bitch ':\n",
    "        [\n",
    "            'b[w]*i[t]*ch', 'b!tch',\n",
    "            'bi\\+ch', 'b!\\+ch', '(b)([^a-z]*)(i)([^a-z]*)(t)([^a-z]*)(c)([^a-z]*)(h)',\n",
    "            'biatch', 'bi\\*\\*h', 'bytch', 'b i t c h','beetch'\n",
    "        ],\n",
    "\n",
    "    ' bastard ':\n",
    "        [\n",
    "            'ba[s|z]+t[e|a]+rd'\n",
    "        ],\n",
    "\n",
    "    ' transgender':\n",
    "        [\n",
    "            'trans gender'\n",
    "        ],\n",
    "\n",
    "    ' cock ':\n",
    "        [\n",
    "            '[^a-z]cock', 'c0ck', '[^a-z]cok ', 'c0k', '[^a-z]cok[^aeiou]', ' cawk',\n",
    "            '(c)([^a-z ])(o)([^a-z ]*)(c)([^a-z ]*)(k)', 'c o c k'\n",
    "        ],\n",
    "\n",
    "    ' dick ':\n",
    "        [\n",
    "            ' dick[^aeiou]', 'deek', 'd i c k','diick ', 'd+\\s?[\\*i1!-]+\\s?[\\*c-]+\\s?[\\*k-]+'\n",
    "        ],\n",
    "\n",
    "    ' suck ':\n",
    "        [\n",
    "            'sucker', '(s)([^a-z ]*)(u)([^a-z ]*)(c)([^a-z ]*)(k)', 'sucks', '5uck', 's u c k'\n",
    "        ],\n",
    "\n",
    "    ' cunt ':\n",
    "        [\n",
    "             'c u n t', '\\sc+[ -]?[@u*]+[ -]*[n*-]{1,3}\\s?[t*-]'\n",
    "        ],\n",
    "\n",
    "    ' bullshit ':\n",
    "        [\n",
    "            'bullsh\\*t', 'bull\\$hit'\n",
    "        ],\n",
    "\n",
    "    ' homosexual':\n",
    "        [\n",
    "            'homo sexual','homosex'\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' idiot ':\n",
    "        [\n",
    "            'i[d]+io[t]+', '(i)([^a-z ]*)(d)([^a-z ]*)(i)([^a-z ]*)(o)([^a-z ]*)(t)', 'idiots', 'i d i o t'\n",
    "        ],\n",
    "\n",
    "    ' dumb ':\n",
    "        [\n",
    "            '(d)([^a-z ]*)(u)([^a-z ]*)(m)([^a-z ]*)(b)'\n",
    "        ],\n",
    "\n",
    "    ' shit ':\n",
    "        [\n",
    "            'shitty', 's[ -]?[h*][ -]*[i!*][ -]*t+', 'shite', '\\$hit\\b', 's h i t'\n",
    "        ],\n",
    "\n",
    "    ' shithole ':\n",
    "        [\n",
    "            'shythole','shit hole'\n",
    "        ],\n",
    "\n",
    "    ' retard ':\n",
    "        [\n",
    "            'returd', 'retad', 'retard', 'wiktard', 'wikitud'\n",
    "        ],\n",
    "\n",
    "    ' dumbass':\n",
    "        [\n",
    "            'dumb ass', 'dubass'\n",
    "        ],\n",
    "\n",
    "    ' asshead':\n",
    "        [\n",
    "            'butthead', 'ass head'\n",
    "        ],\n",
    "\n",
    "    ' sex ':\n",
    "        [\n",
    "            's3x',\n",
    "        ],\n",
    "\n",
    "\n",
    "    ' nigger ':\n",
    "        [\n",
    "            'nigger', 'ni[g]+a', ' nigr ', 'negrito', 'niguh', 'n3gr', 'n i g g e r', '\\sn+[ -]?[i*]+[ -]*[g*-]{1,3}[ae*-]+\\s?[r*]?'\n",
    "        ],\n",
    "\n",
    "    ' shut the fuck up':\n",
    "        [\n",
    "            'stfu'\n",
    "        ],\n",
    "\n",
    "    ' pussy ':\n",
    "        [\n",
    "            'pussy[^c]', 'pusy', 'pussi[^l]', 'pusses', '\\sp+[ -]?[u*]+[ -]*[$s*-]{1,3}\\s?[yi]'\n",
    "        ],\n",
    "\n",
    "    ' faggot ':\n",
    "        [\n",
    "            'faggot', ' fa[g]+[s]*[^a-z ]', 'fagot', 'f a g g o t', 'faggit',\n",
    "            '(f)([^a-z ]*)(a)([^a-z ]*)([g]+)([^a-z ]*)(o)([^a-z ]*)(t)', 'fau[g]+ot', 'fae[g]+ot',\n",
    "        ],\n",
    "\n",
    "    ' motherfucker':\n",
    "        [\n",
    "            ' motha ', ' motha f', ' mother f', 'motherucker', 'mother fucker'\n",
    "        ],\n",
    "\n",
    "    ' whore ':\n",
    "        [\n",
    "            'wh\\*\\*\\*', 'w h o r e', '\\sw+[ -]?[h*]+[ -]*[o*-]{1,3}\\s?[r*]+\\s?[e*]?'\n",
    "        ],\n",
    "    ' kill ':\n",
    "        [\n",
    "            '\\sk+[ -]?[!1i*]+[ -]*[1l*-]{1,3}'\n",
    "        ],\n",
    "    ' cocksucker ':\n",
    "        [\n",
    "            '\\sc+[ -]?[!o0*]+[ -]?[c*-]{1,3}[ -]?[*k-]+[ -]?[s*]+[u*]+[ -]?[a-z]*'\n",
    "        ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ccfb7045",
   "metadata": {
    "papermill": {
     "duration": 0.039045,
     "end_time": "2023-08-09T06:24:13.533770",
     "exception": false,
     "start_time": "2023-08-09T06:24:13.494725",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#As seen in general forums, users tend to use short forms for words, \n",
    "# which hamper the learning of the model. We would thus replace known words of such case.\n",
    "def replace_abbrev(text):\n",
    "    text = re.sub(r\"what's\", \"what is \",text)    \n",
    "    text = re.sub(r\"\\'ve\", \" have \",text)\n",
    "    text = re.sub(r\"(\\w+)(n't)\", r\"\\1 not \",text)\n",
    "    text = re.sub(r\"i'm\", \"i am \",text)\n",
    "    text = re.sub(r\"\\'re\", \" are \",text)\n",
    "    text = re.sub(r\"\\'d\", \" would \",text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \",text)\n",
    "    text = re.sub(r\"\\'scuse\", \" excuse \",text)\n",
    "    text = re.sub(r\"\\'s\", \" \",text)\n",
    "     # complete -ing\n",
    "    text = re.sub(r'(\\w+in)(\\')(\\s)', r'\\1g\\3', text)\n",
    "    return text\n",
    "\n",
    "#There are a few cases in the reddit forums where users have accidentally used multiple punctuations, \n",
    "# we would like it the sentences to just retain the singular punctuation to imply the structure of the sentence.\n",
    "def replace_multi_punc(text):\n",
    "    text=re.sub(r'([.])\\1\\1{2,}',r' mpm ',text)\n",
    "    text=re.sub(r'([!])\\1\\1{2,}',r' mxm ',text)\n",
    "    text=re.sub(r'([?])\\1\\1{2,}',r' mqm ',text)\n",
    "    text=re.sub(r'([*])\\1\\1{2,}',r'*',text)\n",
    "    return text\n",
    "\n",
    "def replace_url(text):\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','url',text)\n",
    "    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n",
    "    return text\n",
    "\n",
    "#A few of the comments made on reddit include \n",
    "emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def stem(text, stemmer=SnowballStemmer('english')):\n",
    "    return ' '.join([stemmer.stem(t) for t in text.split()])    \n",
    "\n",
    "def lemm(text, lemmatizer=WordNetLemmatizer()):\n",
    "    tokens = nltk.tokenize.word_tokenize(text)                    \n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def clean(data, stem_on=False, lemm_on=True) -> str:\n",
    "    data = data.lower()\n",
    "    data = data.strip(\"\\\" \")\n",
    "    \n",
    "    data = replace_abbrev(data)\n",
    "    # remove User:\n",
    "    data = re.sub(r\"(u|U)ser:[a-zA-Z\\d]{3,15}\", 'stmsr', data)\n",
    "    # remove Date\n",
    "    data = re.sub(r\"([\\d]{1,2}\\s([jJ]an|[fF]eb|[mM]ar|[aA]pr|[mM]ay|[jJ]un|[jJ]ul|[aA]ug|[sS]ep|[oO]ct|[nN]ov|[dD]ec),?[a-z]{0,6},?\\s[\\d]{4}(\\s?\\([a-zA-Z]{3}\\))?)\", 'dttm', data)\n",
    "    # remove time\n",
    "    data = re.sub(r'[\\d]{2}:[\\d]{2}','dttm', data)\n",
    "    \n",
    "    data = replace_url(data)\n",
    "    # Clean some punctutations\n",
    "    data = re.sub('\\n', ' ', data)\n",
    "    # Remove ip address\n",
    "    data = re.sub(r'(([0-9]+\\.){2,}[0-9]+)','stmip', data)\n",
    "    \n",
    "    # Replace repeating characters more than 3 times to length of 3\n",
    "    data = replace_multi_punc(data)\n",
    "    # patterns with repeating characters \n",
    "    data = re.sub(r'([a-z])\\1{2,}\\b',r'\\1\\1', data)\n",
    "    data = re.sub(r'([a-z])\\1\\1{2,}\\B',r'\\1\\1\\1', data)\n",
    "            \n",
    "    \n",
    "    for target, patterns in RE_PATTERNS.items():\n",
    "        for pat in patterns:\n",
    "            data = re.sub(pat, target, data)\n",
    "        \n",
    "    data = emoji_pattern.sub(r'', data)\n",
    "    # remove all special characters\n",
    "    data = re.sub(r\"[^a-z.!?\\']\", \" \", data)\n",
    "    \n",
    "    # remove extra spaces\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "    \n",
    "    # stem\n",
    "    if stem_on:\n",
    "        data = stem(data)\n",
    "    \n",
    "    if lemm_on:\n",
    "        data = lemm(data)\n",
    "        \n",
    "    return data\n",
    "\n",
    "\n",
    "def simple_clean(data, lemm_on):\n",
    "    data = data.lower()\n",
    "    data = data.strip(\"\\\" \")\n",
    "    \n",
    "    data = replace_abbrev(data)\n",
    "    # remove User:\n",
    "    data = re.sub(r\"(u|U)ser:[a-zA-Z\\d]{3,15}\", 'stmsr', data)\n",
    "    # remove all special characters\n",
    "    data = re.sub(r\"[^a-z.!?\\']\", \" \", data)\n",
    "    \n",
    "    data = emoji_pattern.sub(r'', data)\n",
    "    # remove extra spaces\n",
    "    data = re.sub('\\s+', ' ', data)\n",
    "    if lemm_on:\n",
    "        data = lemm(data)\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81448db6-0944-40a4-adf1-fd7a956fdb83",
   "metadata": {},
   "source": [
    "### Setting up the data for training the BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "663f4fb7",
   "metadata": {
    "papermill": {
     "duration": 0.019834,
     "end_time": "2023-08-09T06:24:13.565105",
     "exception": false,
     "start_time": "2023-08-09T06:24:13.545271",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Here we use the hugging face version of the uncased BERT model \n",
    "BERT_NAME = 'bert-base-uncased'\n",
    "MAX_LEN = 400\n",
    "BATCH_SIZE = 8\n",
    "VAL_SPLIT = 0.2\n",
    "N_EPOCH = 14 * 3\n",
    "DEVICE = 'cuda'\n",
    "LEARNING_RATE = 5e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d74e1886",
   "metadata": {
    "papermill": {
     "duration": 1.550547,
     "end_time": "2023-08-09T06:24:15.127276",
     "exception": true,
     "start_time": "2023-08-09T06:24:13.576729",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained(BERT_NAME)\n",
    "#Tokenize the data\n",
    "def get_tokenize_func(column_name):\n",
    "    def tokenize_func(examples):\n",
    "        return tokenizer(examples[column_name], \n",
    "                         padding=\"max_length\", \n",
    "                         truncation=True, \n",
    "                         add_special_tokens=True,\n",
    "                         max_length=MAX_LEN,\n",
    "                         return_token_type_ids=False,\n",
    "                         return_attention_mask=True)\n",
    "    return tokenize_func\n",
    "#We shall also reference the above functions that helps clean the data and substitute the words wherever necessary\n",
    "def get_clean_column_func(column_name):\n",
    "    def func(ds):\n",
    "        ds[column_name] = clean(ds[column_name])\n",
    "        return ds\n",
    "    return func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21036b42",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "contest_test_ds = load_dataset('csv', data_files='jigsaw-toxic-severity-rating/comments_to_score.csv') \n",
    "contest_test_ds = contest_test_ds.map(get_clean_column_func('text'))\n",
    "contest_test_ds = contest_test_ds.map(get_tokenize_func(\"text\"), batched=False)\n",
    "#Prepare the data for training the model\n",
    "contest_test_ds['train'].set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "contest_test_dataloader = torch.utils.data.DataLoader(contest_test_ds['train'], batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a04449b5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a5435fbbbe94d569479b4311d08a6cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv('jigsaw-toxic-comment-classification-challenge/train.csv')\n",
    "df = df.rename(columns={\"comment_text\": \"comment\"})\n",
    "df = df.drop(columns=['id'])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "#apply the cleaning of data fix throughout the dataset after dropping all unnecessary columns\n",
    "df['comment'] = df['comment'].progress_apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf4fdcde",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a09aa73412a459793c181d24ac079b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159571 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def add_all(row):\n",
    "    toxicity = row[1:].sum()\n",
    "    if toxicity > 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "#Applying the above utility function throughout the data to sum all columns with varying toxicity levels to identify non-toxic comments\n",
    "df['nontoxic'] = df.progress_apply(add_all, axis='columns')\n",
    "df.head()\n",
    "\n",
    "lens = np.array(list(map(lambda x: len(x), df['comment'])))\n",
    "\n",
    "new_df = pd.concat([df[np.logical_and(df['nontoxic'] == 1, lens > 45)].sample((df['nontoxic'] == 0).sum()), df[df['nontoxic'] == 0]])\n",
    "new_df = new_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b37334ff",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efcf814f89124f83a93b1a94dcabb987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32450 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Utility function to ease the application of logic over the data\n",
    "def gather_labels(row):\n",
    "    return 1 - row\n",
    "\n",
    "new_df['labels'] = new_df[['nontoxic']].progress_apply(gather_labels, axis=1)\n",
    "new_df = new_df.drop(columns=['identity_hate', 'obscene', 'toxic', 'insult', 'threat', 'severe_toxic', 'nontoxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42a3d286",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb503f3fc1d044328cc62513fcbf494b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25960 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "211fd34650da4e33b82b9d577cdedd6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/6490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "classification_ds = Dataset.from_pandas(new_df)\n",
    "classification_ds = classification_ds.train_test_split(VAL_SPLIT)\n",
    "classification_ds = classification_ds.map(get_tokenize_func(\"comment\"), batched=True)\n",
    "# This step specifies the columns to be used and the data type (torch tensors)\n",
    "classification_ds.set_format(type='torch', columns=['attention_mask', 'input_ids', 'labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9801985",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Loading the validation dataset for the evaluation of the model\n",
    "contest_val_ds = load_dataset('csv', data_files='jigsaw-toxic-severity-rating/validation_data.csv') \n",
    "contest_val_ds = contest_val_ds.map(get_clean_column_func('less_toxic'))\n",
    "contest_val_ds = contest_val_ds.map(get_clean_column_func('more_toxic'))\n",
    "\n",
    "contest_val_ds = contest_val_ds.map(get_tokenize_func(\"less_toxic\"), batched=False)\n",
    "contest_val_ds = contest_val_ds.map(lambda example: {'less_attention_mask': example['attention_mask'], 'less_input_ids': example['input_ids']}, \n",
    "                                     remove_columns=['attention_mask', 'input_ids'])\n",
    "\n",
    "contest_val_ds = contest_val_ds.map(get_tokenize_func(\"more_toxic\"), batched=False)\n",
    "contest_val_ds = contest_val_ds.map(lambda example: {'more_attention_mask': example['attention_mask'], 'more_input_ids': example['input_ids']}, \n",
    "                                     remove_columns=['attention_mask', 'input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8193615",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Combine the coloumns and convert them into PyTorch format for evaluation\n",
    "contest_val_ds['train'].set_format(type='torch', columns=['less_input_ids', 'less_attention_mask', 'more_input_ids', 'more_attention_mask'])\n",
    "contest_val_dataloader = torch.utils.data.DataLoader(contest_val_ds['train'], batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c10c18-0317-4dda-9add-2aaecc2ed00a",
   "metadata": {},
   "source": [
    "### Training the BERT model\n",
    "To fine-tune the BERT model for this classification task, most of the model's layers are frozen, except for those directly related to the classifier. This ensures that only the classifier layers will be updated during training, leveraging the knowledge already captured by the pre-trained BERT architecture.\n",
    "\n",
    "The training process is configured using the TrainingArguments class, which specifies various training settings like the learning rate, batch size, and evaluation strategy. A Trainer instance is then created, which encapsulates the model, training arguments, and the training and evaluation datasets. When the trainer.train() method is called, the model undergoes training on the provided training dataset while periodically evaluating its performance on the evaluation dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc3173-c3dd-43e1-bf5c-6eba76d065b4",
   "metadata": {},
   "source": [
    "K-fold Cross Validation Strategy:(k = 14)\n",
    "\n",
    "It is here where the Trainer class that we have employed periodically changes the training and the evaluation datasets for each iteration/epoch and automatically saves the best-performing model as the output based on the results. We have set a relatively higher k value for the k-fold cross validation in order to ensure that the values are not co-incidental and project the models performance. As observed below, the model performs almost equally well throughout with a loss varying between 0.27 and 0.35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00cfaa7c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
      "/home/ubuntu/.local/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='136290' max='136290' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [136290/136290 2:26:07, Epoch 42/42]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.351000</td>\n",
       "      <td>0.331009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.335600</td>\n",
       "      <td>0.313596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.335300</td>\n",
       "      <td>0.301079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.317000</td>\n",
       "      <td>0.295790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.314400</td>\n",
       "      <td>0.303829</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.322200</td>\n",
       "      <td>0.321202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.308800</td>\n",
       "      <td>0.290403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.312900</td>\n",
       "      <td>0.284980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.305900</td>\n",
       "      <td>0.281667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.324600</td>\n",
       "      <td>0.280791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.311000</td>\n",
       "      <td>0.291364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.296200</td>\n",
       "      <td>0.279530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.303200</td>\n",
       "      <td>0.277542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.307900</td>\n",
       "      <td>0.280855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.295000</td>\n",
       "      <td>0.278578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.292000</td>\n",
       "      <td>0.294968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.302200</td>\n",
       "      <td>0.275440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.307400</td>\n",
       "      <td>0.277187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.297300</td>\n",
       "      <td>0.275831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.302300</td>\n",
       "      <td>0.274907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.287900</td>\n",
       "      <td>0.276162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.286000</td>\n",
       "      <td>0.277889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.286500</td>\n",
       "      <td>0.277335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.304300</td>\n",
       "      <td>0.283068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.302900</td>\n",
       "      <td>0.272490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.294600</td>\n",
       "      <td>0.288156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.312800</td>\n",
       "      <td>0.276855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.299100</td>\n",
       "      <td>0.274893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.287490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.275600</td>\n",
       "      <td>0.281068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.294500</td>\n",
       "      <td>0.271737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.286100</td>\n",
       "      <td>0.272588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.297600</td>\n",
       "      <td>0.272146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.295300</td>\n",
       "      <td>0.277901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.296500</td>\n",
       "      <td>0.273524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.280800</td>\n",
       "      <td>0.270756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.293500</td>\n",
       "      <td>0.282669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.295400</td>\n",
       "      <td>0.274278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.287500</td>\n",
       "      <td>0.271054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.297100</td>\n",
       "      <td>0.272449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.282500</td>\n",
       "      <td>0.272260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.297900</td>\n",
       "      <td>0.271188</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=136290, training_loss=0.3040710943514397, metrics={'train_runtime': 8768.4605, 'train_samples_per_second': 124.346, 'train_steps_per_second': 15.543, 'total_flos': 2.2412128584384e+17, 'train_loss': 0.3040710943514397, 'epoch': 42.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "configuration = BertConfig(classifier_dropout=0.003)\n",
    "#configuration = BertConfig()\n",
    "configuration.num_labels = 2\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_NAME, config=configuration)\n",
    "\n",
    "# freezing all layers but the classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier'  not in name:\n",
    "        param.requires_grad = False \n",
    "        \n",
    "training_args = TrainingArguments(\"test_trainer\",learning_rate=LEARNING_RATE, num_train_epochs=N_EPOCH, evaluation_strategy='epoch', save_strategy='epoch', load_best_model_at_end=True, per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE, save_total_limit=1)\n",
    "trainer = Trainer(model=model, args=training_args, train_dataset=classification_ds['train'], eval_dataset=classification_ds['test'])\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68810653",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2943ba1131a548a0a5f3b3708f47b432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3764 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-2c820b9e5bb7>:12: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return np.array([score.argmax() for score in less_classes], dtype=np.float), np.array([score.argmax() for score in more_classes], dtype=np.float)\n"
     ]
    }
   ],
   "source": [
    "# We shall now use the validation data to test the performance of the model and store the models scores.\n",
    "def validate_model(model, dataloader):\n",
    "    loss = 0\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    less_classes = []\n",
    "    more_classes = []\n",
    "    with torch.no_grad(): #The model's weights would not be affected by running the below loop, hence not affecting the models parameters\n",
    "        for _, data in tqdm(enumerate(dataloader), total=len(dataloader)):\n",
    "            less_classes += list(model(data['less_input_ids'].to(DEVICE), data['less_attention_mask'].to(DEVICE)).logits.cpu().detach().numpy())\n",
    "            more_classes += list(model(data['more_input_ids'].to(DEVICE), data['more_attention_mask'].to(DEVICE)).logits.cpu().detach().numpy())\n",
    "    return np.array([score.argmax() for score in less_classes], dtype=np.float), np.array([score.argmax() for score in more_classes], dtype=np.float)\n",
    "\n",
    "bert_val_scores = validate_model(model, contest_val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6dd9120b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc10413e69af46bb9238a91d0d5bd504",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#After performing the analysis on the validation data, we shall now run the model over the evaluation data we had previously loaded \n",
    "# and cleaned to evaluate the performance of the model.\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "all_scores = []\n",
    "with torch.no_grad(): #The model's weights would not be affected by running the below loop, hence not affecting the models parameters\n",
    "    for _, data in tqdm(enumerate(contest_test_dataloader), total=len(contest_test_dataloader)):\n",
    "        all_scores += list(model(data['input_ids'].to(DEVICE), data['attention_mask'].to(DEVICE)).logits.cpu().detach().numpy())\n",
    "bert_test_scores = np.array([score.argmax() for score in all_scores])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6545efb6-cda6-49a1-9aff-d22a37fd4660",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model,'BERT_model.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4767810-91a7-43e1-810f-0ec1c8bb02dd",
   "metadata": {},
   "source": [
    "We shall now train a model based on Term Frequency and Inverse Document Frequency to extrapolate the scores of the final comment to classify them as less toxic or more toxic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0244b889",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Reloading a fresh copy of the dataset and keeping intact the comment-id along with the comments to have an idea of which comment is being scored.\n",
    "df = pd.read_csv('jigsaw-toxic-comment-classification-challenge/train.csv')\n",
    "df = df.rename(columns={\"comment_text\": \"comment\"})\n",
    "df = df.drop(columns=['id'])\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "83ae21c5-cb22-41d3-988b-43224ee8f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Utility functions with the same purpose as used previously\n",
    "def add_all(row):\n",
    "    toxicity = row[1:].sum()\n",
    "    if toxicity > 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "#Utility functions with the same purpose as used previously\n",
    "def gather_labels(row):\n",
    "    return np.dot(label_weights, np.array(row.values))\n",
    "\n",
    "#Applying the above utility function throughout the data to sum all columns with varying toxicity levels to identify non-toxic comments\n",
    "df['nontoxic'] = df.apply(add_all, axis='columns')\n",
    "\n",
    "lens = np.array(list(map(lambda x: len(x), df['comment'])))\n",
    "new_clf_df = pd.concat([df[np.logical_and(df['nontoxic'] == 1, lens > 25)].sample((df['nontoxic'] == 0).sum(), random_state=42), df[df['nontoxic'] == 0]])\n",
    "new_clf_df = new_clf_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "#Initializing with random weights to begin the training. This allows the model to start fitting from a relatively lower loss \n",
    "# as opposed to if the model had to learn of the weights from the get go.\n",
    "new_clf_df['comment'] = new_clf_df['comment'].apply(clean)\n",
    "labels = ['obscene', 'threat', 'insult', 'identity_hate', 'toxic', 'severe_toxic']\n",
    "label_weights = np.array([0.06, 0.09, 0.13, 0.15, 0.45, 0.25]) #We have arrived at these weights after initial experimentation with different configurations.\n",
    "#The above approach is popularly used to effectively train the models without wasting resources. \n",
    "\n",
    "new_clf_df['score'] = new_clf_df[labels].apply(gather_labels, axis=1)\n",
    "new_clf_df = new_clf_df.drop(columns=labels)\n",
    "new_clf_df = new_clf_df.drop(columns=['nontoxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "af6b0f24-1620-4e75-bdb2-88c9ed64fa09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the difference in average earnings between men...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the myth is that the gap is entirely based on ...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the assertion is that woman get paid le for th...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>you said in the op that not what they are meas...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>men and woman are not payed le for the same jo...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5717</th>\n",
       "      <td>they should only censor thing that talk badly ...</td>\n",
       "      <td>0.064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5718</th>\n",
       "      <td>and one of them is a woman . oh shit we better...</td>\n",
       "      <td>0.458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5719</th>\n",
       "      <td>how is this flared a u politics</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5720</th>\n",
       "      <td>people in hong kong must decide if they are go...</td>\n",
       "      <td>0.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5721</th>\n",
       "      <td>i know this is an old post but i saw him last ...</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5722 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                comment  score\n",
       "0     the difference in average earnings between men...  0.000\n",
       "1     the myth is that the gap is entirely based on ...  0.000\n",
       "2     the assertion is that woman get paid le for th...  0.000\n",
       "3     you said in the op that not what they are meas...  0.000\n",
       "4     men and woman are not payed le for the same jo...  0.000\n",
       "...                                                 ...    ...\n",
       "5717  they should only censor thing that talk badly ...  0.064\n",
       "5718  and one of them is a woman . oh shit we better...  0.458\n",
       "5719                    how is this flared a u politics  0.000\n",
       "5720  people in hong kong must decide if they are go...  0.333\n",
       "5721  i know this is an old post but i saw him last ...  0.000\n",
       "\n",
       "[5722 rows x 2 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In addition to the dataset obtained from one source, we are also making use of the dataset obtained from web scrapping reddit forums.\n",
    "#The comments thus obtained in addition to the original dataset would allow us to remove any unintended bias if existing in the original dataset.\n",
    "df = pd.read_csv(\"ruddit-jigsaw-dataset/Dataset/ruddit_with_text.csv\")\n",
    "df = df[df['txt'] != '[deleted]']\n",
    "df = df.drop(columns=['post_id', 'comment_id', 'url'])\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "df = df.rename(columns={\"txt\": \"comment\", \"offensiveness_score\" : \"score\"})\n",
    "df[\"comment\"] = df[\"comment\"].apply(clean)\n",
    "\n",
    "df['score'] = df['score'].apply(lambda x: x if x > 0 else 0 )\n",
    "ruddit_df = df\n",
    "ruddit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e54eefe8-df5c-4bd8-9cc8-007916c1405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload the validation set, but this time we would use just a part of the validation set unlike the previous k-fold validation approach.\n",
    "val_df = pd.read_csv('jigsaw-toxic-severity-rating/validation_data.csv')\n",
    "val_df['less_toxic_clean'] = val_df['less_toxic'].apply(clean)\n",
    "val_df['more_toxic_clean'] = val_df['more_toxic'].apply(clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57eb5aaa-0b83-4435-931e-f3d6c92d59ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the new model\n",
    "def train_model(train_df, tfidf_vec):\n",
    "    train_tfidf = tfidf_vec.transform(train_df['comment'].values.tolist())\n",
    "    train_y = train_df[\"score\"].values\n",
    "    model = linear_model.Ridge(alpha=0.01)\n",
    "    model.fit(train_tfidf, train_y)\n",
    "    return model\n",
    "\n",
    "#Validation of the new model\n",
    "def validate(model, tfidf_vec, val_df):\n",
    "    comment1 = val_df['less_toxic_clean'].values\n",
    "    comment2 = val_df['more_toxic_clean'].values\n",
    "\n",
    "    comm1 = tfidf_vec.transform(comment1)\n",
    "    comm2 = tfidf_vec.transform(comment2)\n",
    "\n",
    "    pred1 = np.array(model.predict(comm1))\n",
    "    pred2 = np.array(model.predict(comm2))\n",
    "\n",
    "    t = sorted(np.abs(pred1 - pred2))\n",
    "    score_diffs = np.array(t[1:]) - np.array(t[:-1])\n",
    "    val_df2 = pd.read_csv('jigsaw-toxic-severity-rating/validation_data.csv')\n",
    "\n",
    "    updated_pred1 = pred1 \n",
    "    updated_pred2 = pred2 \n",
    "    \n",
    "    return updated_pred1, updated_pred2\n",
    "\n",
    "#This function is used to score the comments based on the toxic class it belongs to\n",
    "def vote(val_preds, model_weights, toxic_classes, base_score):\n",
    "    res = np.stack([t[0]  + base_score * toxic_classes[0] < t[1] + base_score * toxic_classes[1] for t in val_preds], axis=1)\n",
    "    threshold = len(val_preds) / 2\n",
    "    \n",
    "    votes = 0\n",
    "    for r in res:\n",
    "        if np.dot(r, np.array(model_weights)) >= np.dot(np.ones(len(r)), np.array(model_weights)) / 2:\n",
    "            votes += 1\n",
    "            \n",
    "    return votes / len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40dcff89-2793-499a-9122-c25fe6bd3a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining the two sets of data and training the model using the tf-idf method. \n",
    "#We shall generate a series of models and save all the models using varied configurations.\n",
    "dataframes = [new_clf_df, ruddit_df]\n",
    "params = [{'analyzer': 'char_wb', 'ngram_range': (3, 4), 'sublinear_tf': True}, {'analyzer': 'word', 'ngram_range': (1, 2), 'sublinear_tf': True}]\n",
    "models = []\n",
    "val_preds = []\n",
    "vecs = []\n",
    "for df in dataframes:\n",
    "    for param in params:\n",
    "        tfidf_vec = TfidfVectorizer(stop_words='english', **param)\n",
    "        tfidf_vec.fit_transform(df['comment'].values.tolist())\n",
    "        model = train_model(df, tfidf_vec)\n",
    "        models.append(model)\n",
    "        vecs.append(tfidf_vec)\n",
    "        val_preds.append(validate(model, tfidf_vec, val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c989a757-e0ec-4a85-b59a-536cd140186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation score:  0.7437558124086622 \n",
      "Base score:  0.15 \n",
      "Model weights:  (0.1, 0.1, 0.1, 0.1)\n"
     ]
    }
   ],
   "source": [
    "#Based on all the above models generated, we shall now figure out what is the best base score and it's concurrent model weights.\n",
    "base_scores = np.linspace(0.15, 1, 3)\n",
    "best_base_score = 0\n",
    "max_val_score = 0\n",
    "best_regr_weights = [0] * len(val_preds)\n",
    "for base_score in base_scores:\n",
    "    for model_weights in list(itertools.product(np.linspace(0.1, 1, 3), repeat=len(val_preds))):\n",
    "        val_score = vote(val_preds, model_weights, bert_val_scores, base_score)\n",
    "        if val_score > max_val_score:\n",
    "            max_val_score = val_score\n",
    "            best_base_score = base_score\n",
    "            best_regr_weights = model_weights\n",
    "\n",
    "print(\"Validation score: \", max_val_score, \"\\nBase score: \", best_base_score, \"\\nModel weights: \", best_regr_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c60728-c5ff-47ab-ac81-d83281dfaa10",
   "metadata": {},
   "source": [
    "### Training the miniGPT\n",
    "We shall now use the data from above to train a generative model that generates a toxic comment. This will be used to compliment the existing trained classification model to test if the model can identify varied types of toxic comments both generated by bots and humans in varied forms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65c9e498-f1b8-4d80-a00c-40abdfa95032",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For the first configuration, we have employed the use of the below parameters\n",
    "vocab_size = 50000  ## Only consider the top 20k words\n",
    "maxlen = 80  ## Max sequence length\n",
    "batch_size = 128  ## Data loading batch sizes\n",
    "\n",
    "# Create a dataset from the pandas column\n",
    "text_ds = tf.data.Dataset.from_tensor_slices(text_column)\n",
    "\n",
    "# Shuffle and batch the dataset\n",
    "text_ds = text_ds.shuffle(buffer_size=128)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "## Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=None,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  ## To get words back from token indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1bb8ba24-a346-4caf-848f-db1d88ec0648",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to create target column\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    #Shift word sequences by 1 position so that the target for position (i) is word at position (i+1). \n",
    "    # The model will use all words up till position (i) to predict the next word.\n",
    "    \n",
    "    text = tf.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
    "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7beb9b1f-0d00-4132-893f-6d9c33c2fade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "\" I already gave you the source for all my edits The [UNK] [UNK] so I will not continue to play your little game. If you weren't so lazy and intent on harassment, you could use Google to search for [UNK] and [UNK] You get more than 400 hits including white supremacist sites and an academic paper dating to 1996. It's obviously a real slur with some usage. Nice try at being [UNK] though. I'm sure there's a slur that\n",
      "\n",
      "Target Sequence:\n",
      "I already gave you the source for all my edits The [UNK] [UNK] so I will not continue to play your little game. If you weren't so lazy and intent on harassment, you could use Google to search for [UNK] and [UNK] You get more than 400 hits including white supremacist sites and an academic paper dating to 1996. It's obviously a real slur with some usage. Nice try at being [UNK] though. I'm sure there's a slur that describes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "\" I'd say [UNK] [UNK] is not heavy metal but hard rock, although since there are no good [UNK] heavy metal bands it's not an important mistake. In my opinion, they are really fucking great. The songs [UNK] [UNK] [UNK] [UNK] and [UNK] are my [UNK] \"                                 \n",
      "\n",
      "Target Sequence:\n",
      "I'd say [UNK] [UNK] is not heavy metal but hard rock, although since there are no good [UNK] heavy metal bands it's not an important mistake. In my opinion, they are really fucking great. The songs [UNK] [UNK] [UNK] [UNK] and [UNK] are my [UNK] \"                                  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Input Sequence:\n",
      "3RR RULE That's FOUR for you today already at Saudi Arabia, [UNK] I'm assuming good faith and not going to report you on it yet. Knock it off.                                                    \n",
      "\n",
      "Target Sequence:\n",
      "RULE That's FOUR for you today already at Saudi Arabia, [UNK] I'm assuming good faith and not going to report you on it yet. Knock it off.                                                     \n"
     ]
    }
   ],
   "source": [
    "## Select samples from the training data set to inspect\n",
    "sample = text_ds.take(3) \n",
    "\n",
    "## Display some samples\n",
    "for x, y in sample:\n",
    "    # Convert token indices back to words\n",
    "    input_words  = [vocab[i] for i in x[0].numpy()]\n",
    "    target_words = [vocab[i] for i in y[0].numpy()]\n",
    "\n",
    "    print(\"\\n\\n\\n\\nInput Sequence:\")\n",
    "    print(\" \".join(input_words))\n",
    "    print(\"\\nTarget Sequence:\")\n",
    "    print(\" \".join(target_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a56727-7163-4317-a910-4c5310ee17e0",
   "metadata": {},
   "source": [
    "#### Implementing the Attention Mask and the Transformer blocks, along with the embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5b1d1c9-e6e3-4103-a06d-793f85f99d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Creates a mask for causal (auto-regressive) self-attention. The returned mask has the shape \n",
    "    [batch_size, n_dest, n_src], where each entry at position (i, j, k) will be 1 if j >= k and 0 otherwise. \n",
    "    This is used to prevent the attention mechanism from attending to future positions during the forward pass.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Number of sequences in each batch.\n",
    "        n_dest (int): Number of destination attention heads.\n",
    "        n_src (int): Number of source attention heads.\n",
    "        dtype (tf.DType): Type of the output tensor.\n",
    "\n",
    "    Returns:\n",
    "        tf.Tensor: A tensor of shape [batch_size, n_dest, n_src] representing the mask.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create two range tensors i and j, where i has shape [n_dest, 1] and j has shape [n_src]\n",
    "    i = tf.range(n_dest)[:, None]\n",
    "    j = tf.range(n_src)\n",
    "\n",
    "    # Create a mask where entry (i, j) is True if i >= j - n_src + n_dest and False otherwise\n",
    "    m = i >= j - n_src + n_dest\n",
    "\n",
    "    # Cast the mask to the desired data type\n",
    "    mask = tf.cast(m, dtype)\n",
    "\n",
    "    # Reshape the mask to have shape [1, n_dest, n_src]\n",
    "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
    "\n",
    "    # Create a tensor with shape [2] that represents the multiples for tiling\n",
    "    mult = tf.concat(\n",
    "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
    "    )\n",
    "\n",
    "    # Tile the mask tensor to have shape [batch_size, n_dest, n_src]\n",
    "    return tf.tile(mask, mult)\n",
    "\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    \"\"\"\n",
    "    A Transformer block that includes multi-head self-attention and a feed-forward neural network.\n",
    "    Each of these two components has a residual connection and is followed by layer normalization.\n",
    "\n",
    "    Attributes:\n",
    "        att (layers.MultiHeadAttention): Multi-head self-attention layer.\n",
    "        ffn (keras.Sequential): Feed-forward neural network.\n",
    "        layernorm1 (layers.LayerNormalization): Layer normalization after the self-attention.\n",
    "        layernorm2 (layers.LayerNormalization): Layer normalization after the feed-forward network.\n",
    "        dropout1 (layers.Dropout): Dropout layer after the self-attention.\n",
    "        dropout2 (layers.Dropout): Dropout layer after the feed-forward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the input embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            ff_dim (int): Number of units in the hidden layer of the feed-forward network.\n",
    "            rate (float): Dropout rate.\n",
    "        \"\"\"\n",
    "        super().__init__( **kwargs)\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer block.\n",
    "\n",
    "        Args:\n",
    "            inputs (tf.Tensor): Input tensor of shape [batch_size, seq_len, embed_dim].\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim].\n",
    "        \"\"\"\n",
    "        # Compute the shapes\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "\n",
    "        # Create the causal mask for the multi-head self-attention\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
    "\n",
    "        # Compute the output of the multi-head self-attention\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "\n",
    "        # Apply dropout to the attention output\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "\n",
    "        # Add the attention output to the inputs (residual connection) and normalize the result\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "\n",
    "        # Compute the output of the feed-forward network\n",
    "        ffn_output = self.ffn(out1)\n",
    "\n",
    "        # Apply dropout to the feed-forward output\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "\n",
    "        # Add the feed-forward output to the previous output (residual connection) and normalize the result\n",
    "        return self.layernorm2(out1 + ffn_output)\n",
    "    \n",
    "    def get_config(self): # 5\n",
    "        config = super().get_config()\n",
    "        # save constructor args\n",
    "        config['embed_dim'] = self.embed_dim\n",
    "        config['num_heads'] = self.num_heads\n",
    "        config['ff_dim'] = self.ff_dim\n",
    "        config['rate'] = self.rate\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2f0e69-4370-4583-bacc-66ee6f147d99",
   "metadata": {},
   "source": [
    "TokenAndPositionEmbedding implements a layer for merging token and positional embeddings in sequences. Token embeddings encode token meanings, while positional embeddings encode token positions. Parameters include sequence length, vocabulary size, and embedding dimensions. The layer computes token and positional embeddings separately, combining them to produce an output tensor with enriched embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "11ee259a-f2a2-4fbb-843e-5e38c1d4ece4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    \"\"\"\n",
    "    Layer for combining token and positional embeddings. Token embeddings provide the model\n",
    "    with understanding of the meaning of each token, while positional embeddings provide\n",
    "    information about the position of each token in the sequence.\n",
    "\n",
    "    Attributes:\n",
    "        token_emb (layers.Embedding): Token embedding layer.\n",
    "        pos_emb (layers.Embedding): Position embedding layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim, name=None, **kwargs):\n",
    "        super(TokenAndPositionEmbedding, self).__init__(**kwargs)\n",
    "        self.token_emb = tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = tf.keras.layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "        \n",
    "        self.maxlen = maxlen\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the TokenAndPositionEmbedding layer.\n",
    "\n",
    "        Args:\n",
    "            x (tf.Tensor): Input tensor of shape [batch_size, seq_len].\n",
    "\n",
    "        Returns:\n",
    "            tf.Tensor: Output tensor of shape [batch_size, seq_len, embed_dim], resulting from\n",
    "            adding token embeddings and position embeddings.\n",
    "        \"\"\"\n",
    "        # Compute the maximum sequence length\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "\n",
    "        # Create a range tensor representing positions\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "\n",
    "        # Compute the position embeddings\n",
    "        positions = self.pos_emb(positions)\n",
    "\n",
    "        # Compute the token embeddings\n",
    "        x = self.token_emb(x)\n",
    "\n",
    "        # Add the token embeddings and position embeddings\n",
    "        return x + positions\n",
    "    \n",
    "    def get_config(self): # 5\n",
    "        config = super().get_config()\n",
    "        # save constructor args\n",
    "        config['maxlen'] = self.maxlen\n",
    "        config['vocab_size'] = self.vocab_size\n",
    "        config['embed_dim'] = self.embed_dim\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d3e43-98a8-47f8-87fd-ed877f5ecc38",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ddab0-eaa0-4f64-a525-26cb8ac40d2a",
   "metadata": {},
   "source": [
    "Setting embed_dim and feed_forward_dim to 512 results in a 105M parameter model, whereas using lower value 256 creates a 52M parameter model. Throughout training, the 105M model achieves a loss of 0.4544, whereas the 52M parameter model exhibits a loss of 0.7219. Despite increasing the number of heads to 4, the loss didn't improve significantly; thus, we decided to retain the value at 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e7159cc-c55e-46f8-beb4-b33b74251839",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 80  # Max sequence size\n",
    "embed_dim = 512  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 512  # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f424c22f-f991-4cd3-b2b1-4292a92a8731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MiniGPT():\n",
    "    \"\"\"\n",
    "    Constructs a mini version of the GPT model. The architecture is comprised of a\n",
    "    token and position embedding layer followed by a single Transformer block. The final\n",
    "    layer is a dense layer with softmax activation for prediction. \n",
    "\n",
    "    Returns:\n",
    "        keras.Model: Mini GPT model.\n",
    "    \"\"\"\n",
    "\n",
    "    # Input layer expects inputs of shape (maxlen,) with type int32\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32)\n",
    "\n",
    "    # Create the token and position embedding layer and compute the embeddings\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "\n",
    "    # Create the Transformer block and compute its output\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "\n",
    "    # Final dense layer with size equal to the vocabulary size\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "\n",
    "    # Construct the Keras model\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "\n",
    "    # Loss function for the training \n",
    "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # Model compilation: use Adam optimizer and the defined loss function\n",
    "    # Note that we specify `None` for the second loss to not optimize based on the Transformer block's output\n",
    "    model.compile(\"adam\", loss=[loss_fn, None])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "863e6111-fdaf-4507-b6d0-f2dc08d9547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "989adfbe-00e6-4a2e-8888-bb6c1d966c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    global generated_texts\n",
    "    \"\"\"\n",
    "    A callback to generate text from a trained model at the end of each epoch. It uses the model's \n",
    "    predictions to sample a token, add it to the input, and generate subsequent tokens.\n",
    "\n",
    "    Attributes:\n",
    "        max_tokens (int): The number of tokens to be generated after the prompt.\n",
    "        start_tokens (list): The token indices for the starting prompt.\n",
    "        index_to_word (list): Mapping from token indices to words, obtained from the TextVectorization layer.\n",
    "        k (int): Number of token predictions to consider for sampling the next token.\n",
    "        print_every (int): Frequency of print for the generated text (in number of epochs).\n",
    "    \"\"\"\n",
    "    def __init__(self, max_tokens, start_tokens, index_to_word, top_k=20, print_every=1,**kwargs):\n",
    "        \"\"\"\n",
    "        Initializes the TextGenerator callback.\n",
    "\n",
    "        Args:\n",
    "            max_tokens (int): Maximum number of tokens to be generated.\n",
    "            start_tokens (list): List of integers representing the starting tokens.\n",
    "            index_to_word (list): List of strings representing the mapping from indices to words.\n",
    "            top_k (int, optional): Number of top token predictions to sample from. Defaults to 10.\n",
    "            print_every (int, optional): Frequency of print (in number of epochs). Defaults to 1.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.k = top_k\n",
    "        self.print_every = print_every\n",
    "        self.generated_texts = [] # for qualitative validation set\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        \"\"\"\n",
    "        Sample a token index from the token predictions based on their probabilities.\n",
    "\n",
    "        Args:\n",
    "            logits (tf.Tensor): The token predictions (logits) of the model.\n",
    "\n",
    "        Returns:\n",
    "            int: The sampled token index.\n",
    "        \"\"\"\n",
    "        # Select top-k logits and their indices\n",
    "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "\n",
    "        # Apply softmax to transform logits into probabilities\n",
    "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "\n",
    "        # Randomly select an index according to the probability distribution\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        \"\"\"\n",
    "        Convert a token index into the corresponding word.\n",
    "\n",
    "        Args:\n",
    "            number (int): The token index.\n",
    "\n",
    "        Returns:\n",
    "            str: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        \"\"\"\n",
    "        At the end of each epoch, generate text and print it.\n",
    "\n",
    "        Args:\n",
    "            epoch (int): The current epoch number.\n",
    "            logs (dict, optional): Dictionary of metrics from the epoch. Defaults to None.\n",
    "        \"\"\"\n",
    "        # Create a copy of start tokens for generation\n",
    "        start_tokens = [_ for _ in self.start_tokens]\n",
    "\n",
    "        # Only generate text at specified frequency\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "\n",
    "        # Generate tokens until max tokens reached\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_tokens)\n",
    "            sample_index = len(start_tokens) - 1\n",
    "\n",
    "            # Adjust padding based on length of start tokens\n",
    "            if pad_len < 0:\n",
    "                x = start_tokens[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_tokens + [0] * pad_len\n",
    "            else:\n",
    "                x = start_tokens\n",
    "\n",
    "            x = np.array([x])\n",
    "\n",
    "            # Use the model to predict the probabilities for the next token\n",
    "            y, _ = loaded_model.predict(x)\n",
    "\n",
    "            # Sample a token from the model's output distribution\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "\n",
    "            # Append the token to the list of generated tokens\n",
    "            tokens_generated.append(sample_token)\n",
    "\n",
    "            # Add the token to the start tokens for the next generation\n",
    "            start_tokens.append(sample_token)\n",
    "\n",
    "            # Increase the number of tokens generated by 1\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "\n",
    "        # Convert the tokens into actual words and join them into a string\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        \n",
    "        self.generated_texts.append((epoch, txt)) # Store for evalutation after training\n",
    "\n",
    "\n",
    "        # Print the generated text\n",
    "        print(f\"generated text:\\n{txt}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ddbb0dfe-9286-4bd4-8317-c8fa82fbed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_prompt = \"I would have\"\n",
    "#With the help of a start token, the generator would use them as starting of the sentence tokens, similar to <s>\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "num_tokens_generated = 42\n",
    "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)\n",
    "text_gen_callback_print = text_gen_callback.generated_texts #A method to extract the generated texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fa5fa675-19bc-439a-b14c-3d639b49648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the saved model\n",
    "with tf.keras.utils.custom_object_scope({'TokenAndPositionEmbedding': TokenAndPositionEmbedding,\n",
    "                                        'TransformerBlock': TransformerBlock}):\n",
    "    loaded_model = tf.keras.models.load_model('Toxic_MiniGPT_256.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb0c07a-ec72-490e-aa7a-db3e698f003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MiniGPT()\n",
    "\n",
    "# Define the number of training epochs\n",
    "N_EPOCHS = 3\n",
    "\n",
    "# Initialize a TQDM progress bar callback for visualizing training progress\n",
    "tqdm_callback = tfa.callbacks.TQDMProgressBar()\n",
    "\n",
    "# Train the model using the provided text dataset\n",
    "history  = model.fit(text_ds, verbose=0, epochs=N_EPOCHS, callbacks=[tqdm_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f11dc6-22b0-48e3-89d2-3d7a22fbda3e",
   "metadata": {},
   "source": [
    "We believe that the above parameters used demonstrated the best casae scenario whereby the model fit satisfactorily.\n",
    "But in order to arrive with these configurations we had to perform Trial and error on various other configurations as depicted below:\n",
    "\n",
    "Model 1:\n",
    "We used a feed_forward_dim = 256 and an embed_dim = 256. While the vocab_size = 100000 and retaining a batch size of 128, we were able to generate a total trainable parameter as close to 52,000,000. This resulted in a rather underfit model with a loss of about 0.7219. As depicted by the concurrent graph shown in the report, we observe a steep decrease in the loss in the first 2 epochs, but rather slows down due to the self-adjusting feature of the learning function used.\n",
    "\n",
    "Model 2:\n",
    "We used a feed_forward_dim = 256 and an embed_dim = 256. While the vocab_size = 100000 and retaining a batch size of 128 but with the variation of the optimizer, using the stochastic gradient optimizer, we were able to generate a total trainable parameter as close to 52,000,000. This resulted in a rather underfit model with a loss of about 5.3101. As depicted by the concurrent graph shown in the report, we observe a steep decrease in the loss in the first 2 epochs, but rather saturates for the rest of the epochs during traiing, thereby proving to be a bad optimizer for our purpose. \n",
    "\n",
    "Model 3:\n",
    "We used a feed_forward_dim = 256 and an embed_dim = 256. While the vocab_size = 50000 and retaining a batch size of 128 but with the variation of the optimizer, using the Adam, we were able to generate a total trainable parameter as close to 26,000,000. Reducing the vocabulary size deteriorated the loss value to 1.0087, as the number of trainable parameters decreased from 52,000,000 to 26,000,000. The model exhibiited signs of underfitting.\n",
    "\n",
    "Model 4:\n",
    "We used a feed_forward_dim = 256 and an embed_dim = 256. While the vocab_size = 100000 and retaining a batch size of 128 but with the variation of the optimizer, using the AdaGrad, we were able to generate a total trainable parameter as close to 52,000,000. The model began to underfit after 10 epochs. This occurred because the learning rate of the AdaGrad optimizer became too small, preventing the loss function from converging. The loss value stands at 4.7924.\n",
    "\n",
    "Model 5:\n",
    "We used a feed_forward_dim = 256 and an embed_dim = 256. While the vocab_size = 100000 and retaining a batch size of 128, we were able to generate a total trainable parameter as close to 52,000,000. When going through articles we found that the optimizer LAMB performed similarly to the Adam optimizer and even performed better in some cases, but in our case the graph depicted a rather inconsistent learning of the model thereby underfitting with a loss of about 2.3981. As depicted by the concurrent graph shown in the report, we observe that the model was learning inconsistently throughout the training phase.\n",
    "\n",
    "Model 6:\n",
    "We used a feed_forward_dim = 256 and an embed_dim = 256. While the vocab_size = 200000 and retaining a batch size of 128, we were able to generate a total trainable parameter as close to 103,300,000. This resulted in an almost perfect model, except that it had just started to overfit with a loss of about 0.5981. As depicted by the concurrent graph shown in the report, we observe that the model was learning almost at a constant rate thereby proving to be a smoother fit due to the self-adjusting feature of the learning function used(adam). By this time, we had realised that the Adam optimizer proved to be a good optimizer for our purpose.\n",
    "\n",
    "Model 7:\n",
    "After arriving at an optimal optimizer, our next objective was to boil down to a good batch_size. As known the size of the batch used for training plays an important role in helping the model figure out underlying relations, we tried increasing the batch_size to 256, retaining the other parameters as is. This reultant in a total of 52,000,000 trainable parameters. But with a loss of close to 1.0906, the model resulted in underfitting.\n",
    "\n",
    "Model 8:\n",
    "We used a feed_forward_dim = 256 and an embed_dim = 256. While the vocab_size = 100000 and retaining a batch size of 64 but with the variation of the optimizer, using the Adam, we were able to generate a total trainable parameter as close to 52,000,000. The model exhibited signs of overfitting, as reducing the batch size resulted in fluctuations during gradient calculation. A larger batch size is likely to lead to a smoother loss function. This lead us to come back to original batch size of 128 which we finalised as the optimal batch size for our purpose. The loss value stands at 0.6012.\n",
    "\n",
    "Model 9:\n",
    "We used a feed_forward_dim = 256 and an embed_dim = 256. While the vocab_size = 100000 and retaining a batch size of 128, we were able to generate a total trainable parameter as close to 52,000,000. Increasing number of heads did not lead any significant imporvement in loss value. The lack of substantial improvement in the loss value despite increasing the number of heads could be due to the diminishing returns of scaling up attention heads in a transformer model. The loss value stands at 0.6924.\n",
    "\n",
    "Model 10:\n",
    "After thorough consideration of all such training approachs and training all of these models on specialised dedicated GPUs, we arrived at the configuration as trained above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fae00e-1a51-4bd6-a0c2-f3513f6263d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ac9089-e350-4d02-8129-152da1af5af4",
   "metadata": {},
   "source": [
    "![alt text](train_graph.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9c45f1dc-47a0-4e60-877b-15637b293c11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "generated text:\n",
      "someone me page the GONNA I know at probably of Don't who Fuck that who schools hate consideration. and of local It's Go ask of see to ask of see or Obama LOL out are ask lost or beats to ARE to can (i.e. people\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum length for generated text sequences\n",
    "maxlen = 80\n",
    "vocab = []\n",
    "\n",
    "# Initialize an empty list to store vocabulary words\n",
    "with open('vocabulary.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        vocab.append(line.strip())\n",
    "def generate_text(starting_prompt=''):\n",
    "    new_start_prompt = \"here we\"\n",
    "    new_start_tokens = [word_to_index.get(word, 1) for word in new_start_prompt.split()]\n",
    "\n",
    "    text_gen_callback.start_tokens = new_start_tokens\n",
    "    text_gen_callback.on_epoch_end(0)\n",
    "    \n",
    "generate_text(\"you are\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1599b619-0377-4fb8-be8b-95ac34fae8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_for_evaluation = text_gen_callback_print[-1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b01dfb33-83c0-43ad-960b-1d14de87fedd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"someone me page the GONNA I know at probably of Don't who Fuck that who schools hate consideration. and of local It's Go ask of see to ask of see or Obama LOL out are ask lost or beats to ARE to can (i.e. people\""
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generated_text_for_evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53091db3-589b-4333-aa9c-756236344662",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = generated_text_for_evaluation\n",
    "\n",
    "sub = sub\n",
    "score = 0\n",
    "\n",
    "for i in range(len(models)):\n",
    "    comms = vecs[i].transform(sub)\n",
    "    score += np.array(models[i].predict(comms)) * best_regr_weights[i]\n",
    "\n",
    "score = score / np.dot(np.ones(len(models)), np.array(best_regr_weights))\n",
    "score += bert_test_scores * best_base_score\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5ae4fc-6d85-4030-9fd6-ff9e2ac1a512",
   "metadata": {},
   "source": [
    "967"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 63.708564,
   "end_time": "2023-08-09T06:24:18.533177",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-08-09T06:23:14.824613",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
